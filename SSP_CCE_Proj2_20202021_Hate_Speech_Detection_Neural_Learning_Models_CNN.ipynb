{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSP_CCE_Proj2_20202021_Hate_Speech_Detection_Neural_Learning_Models_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrCuCYE3b7Zs"
      },
      "source": [
        "# Hate speech Detection using CNN\n",
        "In this notebook, we conduct a preliminary experiment on the detection of hate speech in Arabic tweets as part of our participation in the Hate Speech Detection subtask in [OSACT4 workshop](http://edinburghnlp.inf.ed.ac.uk/workshops/OSACT4/).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TwgKlI1te-w",
        "outputId": "d54b96e1-3a44-4696-94ed-7e9ab816b86a"
      },
      "source": [
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnSw0od87C8-"
      },
      "source": [
        "from keras.layers import Embedding, Dense, Dropout, Input, LSTM, Bidirectional,GRU\n",
        "from keras.layers import MaxPooling1D, Conv1D, Flatten\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.models import Model\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn import preprocessing\n",
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import csv\n",
        "\n",
        "from keras import optimizers\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import (\n",
        "    classification_report as creport\n",
        ")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6_dt2cDtobO"
      },
      "source": [
        "# Data and AraVec2.0 (pre-trained word embeddings model) Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mn0KDJg-o5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9164e7-4f0a-4fc6-bd52-469645a5376b"
      },
      "source": [
        "#pre-trained word embedding: https://github.com/bakrianoo/aravec/tree/master/AraVec%202.0\n",
        "\"\"\"\n",
        "Citation:\n",
        "Abu Bakr Soliman, Kareem Eisa, and Samhaa R. El-Beltagy, “AraVec:\n",
        "A set of Arabic Word Embedding Models for use in Arabic NLP”,\n",
        "in proceedings of the 3rd International Conference on \n",
        "Arabic Computational Linguistics (ACLing 2017), Dubai, UAE, 2017.\n",
        "\"\"\"\n",
        "! unzip '/content/drive/My Drive/tweets_sg_300.zip'  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/tweets_sg_300.zip\n",
            "  inflating: tweets_sg_300           \n",
            "  inflating: tweets_sg_300.trainables.syn1neg.npy  \n",
            "  inflating: tweets_sg_300.wv.vectors.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3kKX6Ra-p1d"
      },
      "source": [
        "# Word_embedding_path\n",
        "embedding_path = '/content/tweets_sg_300'           #Twitter-Skipgram model-300d(trained on 77,600,000 Arabic tweets)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpRSY6aVACVF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "0bf4b5c0-48e1-4e27-91c5-20d704a4decf"
      },
      "source": [
        "train_data = pd.read_csv('/content/drive/My Drive/train_data.csv')\n",
        "train_data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>Hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>الحمدلله يارب فوز مهم يا زمالك.. كل الدعم ليكم...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>فدوه يا بخت فدوه يا زمن واحد منكم يجيبه</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RT @USER: يا رب يا واحد يا أحد بحق يوم الاحد ا...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @USER: #هوا_الحرية يا وجع قلبي عليكي يا امي...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>يا بكون بحياتك الأهم يا إما ما بدي أكون 🎼</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6995</th>\n",
              "      <td>@USER يا حمار ، يا جاهل ، نسبة الباطل ما بتتحس...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6996</th>\n",
              "      <td>RT @USER: @USER كل زق يا طاقية يا واطي يا حقير...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6997</th>\n",
              "      <td>@USER&lt;LF&gt;يا كبير يا ممتع يا نجم لابد أن تعي جي...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6998</th>\n",
              "      <td>يا رب الاتحاد يفوز يا رب. 😭😭 #الاتحاد_النصر</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6999</th>\n",
              "      <td>يعز عليا ادخل الشارع وملاقيكش واقف مستنيني في ...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Tweet Offensive    Hate\n",
              "0     الحمدلله يارب فوز مهم يا زمالك.. كل الدعم ليكم...   NOT_OFF  NOT_HS\n",
              "1               فدوه يا بخت فدوه يا زمن واحد منكم يجيبه   NOT_OFF  NOT_HS\n",
              "2     RT @USER: يا رب يا واحد يا أحد بحق يوم الاحد ا...       OFF      HS\n",
              "3     RT @USER: #هوا_الحرية يا وجع قلبي عليكي يا امي...   NOT_OFF  NOT_HS\n",
              "4             يا بكون بحياتك الأهم يا إما ما بدي أكون 🎼   NOT_OFF  NOT_HS\n",
              "...                                                 ...       ...     ...\n",
              "6995  @USER يا حمار ، يا جاهل ، نسبة الباطل ما بتتحس...       OFF  NOT_HS\n",
              "6996  RT @USER: @USER كل زق يا طاقية يا واطي يا حقير...       OFF  NOT_HS\n",
              "6997  @USER<LF>يا كبير يا ممتع يا نجم لابد أن تعي جي...   NOT_OFF  NOT_HS\n",
              "6998        يا رب الاتحاد يفوز يا رب. 😭😭 #الاتحاد_النصر   NOT_OFF  NOT_HS\n",
              "6999  يعز عليا ادخل الشارع وملاقيكش واقف مستنيني في ...   NOT_OFF  NOT_HS\n",
              "\n",
              "[7000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5frVvfiL72D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "69420dec-4600-4290-c18b-bcb8393ca059"
      },
      "source": [
        "dev_data = pd.read_csv('/content/drive/My Drive/dev_data.csv')\n",
        "dev_data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>Hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>فى حاجات مينفعش نلفت نظركوا ليها زى الاصول كده...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RT @USER: وعيون تنادينا تحايل فينا و نقول يا ع...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>يا بلادي يا أم البلاد يا بلادي بحبك يا مصر بحب...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @USER: يا رب يا قوي يا معين مدّني بالقوة و ...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @USER: رحمك الله يا صدام يا بطل ومقدام. URL</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>RT @USER: انتو بتوزعوا زيت وسكر فعلا يا عباس؟&lt;...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>RT @USER: كدا يا عمر متزعلهاش يا حبيبي 😂 URL</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>هدا سكن اطفال امارتين من شارقة طالبين فزعتكم ي...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>RT @USER: ومدني بمدد من قوتك أواجه به ضعفي.. و...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>يا سلااااام يا يو خالد انت والطرب الاصيل URL</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Tweet Offensive    Hate\n",
              "0    فى حاجات مينفعش نلفت نظركوا ليها زى الاصول كده...   NOT_OFF  NOT_HS\n",
              "1    RT @USER: وعيون تنادينا تحايل فينا و نقول يا ع...   NOT_OFF  NOT_HS\n",
              "2    يا بلادي يا أم البلاد يا بلادي بحبك يا مصر بحب...   NOT_OFF  NOT_HS\n",
              "3    RT @USER: يا رب يا قوي يا معين مدّني بالقوة و ...   NOT_OFF  NOT_HS\n",
              "4       RT @USER: رحمك الله يا صدام يا بطل ومقدام. URL   NOT_OFF  NOT_HS\n",
              "..                                                 ...       ...     ...\n",
              "995  RT @USER: انتو بتوزعوا زيت وسكر فعلا يا عباس؟<...   NOT_OFF  NOT_HS\n",
              "996       RT @USER: كدا يا عمر متزعلهاش يا حبيبي 😂 URL   NOT_OFF  NOT_HS\n",
              "997  هدا سكن اطفال امارتين من شارقة طالبين فزعتكم ي...   NOT_OFF  NOT_HS\n",
              "998  RT @USER: ومدني بمدد من قوتك أواجه به ضعفي.. و...   NOT_OFF  NOT_HS\n",
              "999       يا سلااااام يا يو خالد انت والطرب الاصيل URL   NOT_OFF  NOT_HS\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9yxPtQjKsPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79622fae-0b4e-4947-8fa1-7503fa4dd578"
      },
      "source": [
        "print(\"Train data shape: {} \\nDev data shape: {}\".format(train_data.shape,dev_data.shape))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape: (7000, 3) \n",
            "Dev data shape: (1000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "rJB9yBbU5443",
        "outputId": "2701b6f3-32e1-4cd9-eb42-565bbb8f910b"
      },
      "source": [
        "test_data = pd.read_csv('/content/drive/My Drive/Test_data.csv')\r\n",
        "test_data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>Hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@USER اما انت تقعد طول عمرك لا مبدا ولا راي ثا...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@USER @USER بتخاف نسوانك يزعلوا ولا ايه 😂 اه ي...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RT @USER: يا عـسانـى نـبـقى يا عـمري حـبايـب و...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @USER: باقي البيان وينو ما شفنه يا برهان &lt;L...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@USER @USER اللهم انت الشافي المعافي اشفيه وجم...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>RT @USER: الله لايوفقك يا مهند عسيري يا معوق و...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>RT @USER: @USER حبيبي يا يوسف وانت طيب يا صاحب...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>RT @USER: يا بو محمد عشت يا طيب الفال&lt;LF&gt;عاشت ...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>أنا مستني الحلقة بقالي سنتين يا بضان يا ابن ال...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>انتظروا العقوبة الالهية يا من تدعمون الارهاب ي...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Tweet Offensive    Hate\n",
              "0     @USER اما انت تقعد طول عمرك لا مبدا ولا راي ثا...       OFF      HS\n",
              "1     @USER @USER بتخاف نسوانك يزعلوا ولا ايه 😂 اه ي...       OFF  NOT_HS\n",
              "2     RT @USER: يا عـسانـى نـبـقى يا عـمري حـبايـب و...   NOT_OFF  NOT_HS\n",
              "3     RT @USER: باقي البيان وينو ما شفنه يا برهان <L...       OFF  NOT_HS\n",
              "4     @USER @USER اللهم انت الشافي المعافي اشفيه وجم...   NOT_OFF  NOT_HS\n",
              "...                                                 ...       ...     ...\n",
              "1995  RT @USER: الله لايوفقك يا مهند عسيري يا معوق و...       OFF  NOT_HS\n",
              "1996  RT @USER: @USER حبيبي يا يوسف وانت طيب يا صاحب...   NOT_OFF  NOT_HS\n",
              "1997  RT @USER: يا بو محمد عشت يا طيب الفال<LF>عاشت ...   NOT_OFF  NOT_HS\n",
              "1998  أنا مستني الحلقة بقالي سنتين يا بضان يا ابن ال...       OFF  NOT_HS\n",
              "1999  انتظروا العقوبة الالهية يا من تدعمون الارهاب ي...       OFF  NOT_HS\n",
              "\n",
              "[2000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR9nZxKv6DcW"
      },
      "source": [
        "def get_embedding_matrix(word_index, embedding_index, vocab_dim):\n",
        "    print('Building embedding matrix...')\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, vocab_dim))\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_matrix[i] = embedding_index.get_vector(word)\n",
        "        except:\n",
        "            pass\n",
        "    print('Embedding matrix built.') \n",
        "    #print(\"Word index\", word_index.items())\n",
        "    #print(embedding_matrix) \n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def get_init_parameters(path, ext=None):\n",
        "    if ext == 'vec':\n",
        "        word_model = KeyedVectors.load_word2vec_format(path).wv\n",
        "    else:\n",
        "        word_model = KeyedVectors.load(path).wv\n",
        "    n_words = len(word_model.vocab)\n",
        "    vocab_dim = word_model[word_model.index2word[0]].shape[0]\n",
        "    index_dict = dict()\n",
        "    for i in range(n_words):\n",
        "        index_dict[word_model.index2word[i]] = i+1\n",
        "    print('Number of words in the word embedding',n_words)\n",
        "    #print('word_model', word_model)\n",
        "    #print(\"index_dict\",index_dict)\n",
        "    return word_model, index_dict, n_words, vocab_dim\n",
        "\n",
        "def get_max_length(text_data, return_line=False):\n",
        "    max_length = 0\n",
        "    long_line = \"\"\n",
        "    for line in text_data:\n",
        "        new = len(line.split())\n",
        "        if new > max_length:\n",
        "            max_length = new\n",
        "            long_line = line\n",
        "    if return_line:\n",
        "        return long_line, max_length\n",
        "    else:\n",
        "        return max_length\n",
        "    print(\"max\",long_line,max_length)\n",
        "\n",
        "def load_datasets(data_paths, header=True):\n",
        "    x = []\n",
        "    y = []\n",
        "    for data_path in data_paths:\n",
        "        with open(data_path, 'r') as f:\n",
        "            for line in f:\n",
        "                if header:\n",
        "                    header = False\n",
        "                else:\n",
        "                    temp = line.split(',')\n",
        "                    x.append(temp[0])\n",
        "                    y.append(temp[2].replace('\\n', ''))\n",
        "    max_length = get_max_length(x)\n",
        "    print('Max length:', max_length)\n",
        "    return x,y, max_length\n",
        "\n",
        "def get_train_test(train_raw_text, dev_raw_text, test_raw_text, n_words, max_length):\n",
        "    tokenizer = text.Tokenizer(num_words=n_words)\n",
        "    tokenizer.fit_on_texts(list(train_raw_text))\n",
        "    word_index = tokenizer.word_index\n",
        "   \n",
        "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
        "    dev_tokenized = tokenizer.texts_to_sequences(dev_raw_text)\n",
        "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
        "\n",
        "    return sequence.pad_sequences(train_tokenized, maxlen=max_length, padding='post', truncating='post'),\\\n",
        "           sequence.pad_sequences(dev_tokenized, maxlen=max_length, padding='post', truncating='post'),\\\n",
        "           sequence.pad_sequences(test_tokenized, maxlen=max_length, padding='post', truncating='post'),\\\n",
        "           word_index\n",
        "\n",
        "def class_str_2_ind(x_train,x_dev, x_test, y_train,y_dev, y_test, classes, n_words, max_length):\n",
        "    print('Converting data to trainable form...')\n",
        "    y_encoder = preprocessing.LabelEncoder()\n",
        "    y_encoder.fit(classes)\n",
        "    y_train = y_encoder.transform(y_train)\n",
        "    y_dev = y_encoder.transform(y_dev)\n",
        "    y_test = y_encoder.transform(y_test)\n",
        "\n",
        "    #print(y_train)\n",
        "    #print(y_test)\n",
        "    train_y_cat = np_utils.to_categorical(y_train, len(classes))\n",
        "    x_vec_train, x_vec_dev, x_vec_test, word_index = get_train_test(x_train,x_dev, x_test, n_words, max_length)\n",
        "    print('Number of training examples: ' + str(len(x_vec_train)))\n",
        "    print('Number of dev examples: ' + str(len(x_vec_test)))\n",
        "    return x_vec_train,x_vec_dev, x_vec_test, y_train, y_dev, y_test, train_y_cat, word_index\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT0r5MyUbtGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b63a9f-38ed-4596-ada0-f03b1161573c"
      },
      "source": [
        "WORD_MODEL, _, MAX_FEATURES, EMBED_SIZE = get_init_parameters(embedding_path) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in the word embedding 331679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjwJ3aN3U1Mv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1022b7-ebb3-42b0-811c-63d89f46e974"
      },
      "source": [
        "# load train data\n",
        "train_data_path=[\"/content/drive/My Drive/train_data_cleaned.csv\"]\n",
        "x_train, y_train, MAX_TEXT_LENGTH = load_datasets(train_data_path)\n",
        "CLASSES_LIST = np.unique(y_train)\n",
        "print('Label categories: ' + str(CLASSES_LIST))\n",
        "#0= HS, 1= NOT_HS"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length: 84\n",
            "Label categories: ['HS' 'NOT_HS']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGShWPV6UwCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67816463-5fba-457a-bbb6-1cdb082141a2"
      },
      "source": [
        "# load dev data\n",
        "dev_data_path=[\"/content/drive/My Drive/dev_data_cleaned.csv\"]\n",
        "x_dev, y_dev, MAX_TEXT_LENGTH = load_datasets(dev_data_path)\n",
        "CLASSES_LIST = np.unique(y_dev)\n",
        "print('Label categories: ' + str(CLASSES_LIST))\n",
        "#0= HS, 1= NOT_HS"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length: 72\n",
            "Label categories: ['HS' 'NOT_HS']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqlf4vZ76KLP",
        "outputId": "aa150d05-ea31-423d-afe9-979a9150a30b"
      },
      "source": [
        "# load test data\r\n",
        "test_data_path=[\"/content/drive/My Drive/test_data_cleaned.csv\"]\r\n",
        "x_test, y_test, MAX_TEXT_LENGTH = load_datasets(test_data_path)\r\n",
        "CLASSES_LIST = np.unique(y_test)\r\n",
        "print('Label categories: ' + str(CLASSES_LIST))\r\n",
        "#0= HS, 1= NOT_HS"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length: 72\n",
            "Label categories: ['HS' 'NOT_HS']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcpaFQRJV8YL"
      },
      "source": [
        "MAX_TEXT_LENGTH=84"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plOnSpgUb18i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd9386f-8a1d-4f8d-e1b2-01b5a73c8c52"
      },
      "source": [
        "x_train, x_dev,x_test, y_train, y_dev, y_test, train_y_cat, word_index = class_str_2_ind(x_train, x_dev,x_test, \n",
        "                                                                            y_train, y_dev,y_test,\n",
        "                                                                            CLASSES_LIST, MAX_FEATURES,\n",
        "                                                                            MAX_TEXT_LENGTH)\n",
        "dev_cat_y = np_utils.to_categorical(y_dev, len(CLASSES_LIST))\n",
        "test_cat_y = np_utils.to_categorical(y_test, len(CLASSES_LIST))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converting data to trainable form...\n",
            "Number of training examples: 7000\n",
            "Number of dev examples: 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQiq-BKShbLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371c2590-f9be-4153-a87e-56adc629a0da"
      },
      "source": [
        "print(\"Tokens number: \"+str(len(word_index)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens number: 30103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjJd-CWUiNlQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa0fc6b-3e50-4ab1-a142-1ee722c1dc96"
      },
      "source": [
        "# Sequence length\n",
        "print(\"Original sequence length: \"+str(MAX_TEXT_LENGTH))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original sequence length: 84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXB7MDh9lLcM"
      },
      "source": [
        "#  CNN model building:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRIAZtDUidul"
      },
      "source": [
        "def get_model(embedding_weights, word_index, vocab_dim, max_length, print_summary=True):\n",
        "    \"\"\"\n",
        "    Create Neural Network With an Embedding layer\n",
        "    \"\"\"\n",
        "    inp = Input(shape=(max_length,))\n",
        "    model = Embedding(input_dim=len(word_index)+1,\n",
        "                      output_dim=vocab_dim,\n",
        "                      trainable=False,\n",
        "                      weights=[embedding_weights])(inp)\n",
        "\n",
        "    model = Conv1D(filters=25, kernel_size=5, padding='same', activation='relu')(model)\n",
        "    model = MaxPooling1D(pool_size=2)(model)\n",
        "    model = Flatten()(model)\n",
        "   \n",
        "    model = Dense(2, activation='sigmoid')(model)\n",
        "    model = Model(inputs=inp, outputs=model)\n",
        "    \n",
        "    from keras import optimizers\n",
        "\n",
        "    opt = optimizers.Adam(lr=0.0001)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    if print_summary:\n",
        "        model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH):\n",
        "    tmp = get_embedding_matrix(word_index, WORD_MODEL, EMBED_SIZE)\n",
        "    model = get_model(tmp, word_index, EMBED_SIZE, MAX_TEXT_LENGTH, print_summary=True)\n",
        "    return model\n",
        "\n",
        "\n",
        "class TestCallback(Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        x, y = self.test_data\n",
        "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
        "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
        "\n",
        "def train_fit_predict(model, x_train, x_dev, y_train, y_dev, batch_size, epochs, TestCallback=TestCallback):\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs, verbose=1,\n",
        "                        validation_data=(x_dev, y_dev),\n",
        "                        callbacks=[TestCallback((x_dev, y_dev))])\n",
        "    return history, model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKpEhF4ljGqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d7b591-1751-4a0e-a47b-de3135b636fa"
      },
      "source": [
        "model = get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building embedding matrix...\n",
            "Embedding matrix built.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 84)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 84, 300)           9031200   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 84, 25)            37525     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 42, 25)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1050)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 2102      \n",
            "=================================================================\n",
            "Total params: 9,070,827\n",
            "Trainable params: 39,627\n",
            "Non-trainable params: 9,031,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIdYkbZzjJpe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dda89fb-7d03-434d-fd1a-3f5751f35be8"
      },
      "source": [
        "time_start = time()\n",
        "history, model = train_fit_predict(model,\n",
        "                               x_train[:, :MAX_TEXT_LENGTH],\n",
        "                               x_dev[:, :MAX_TEXT_LENGTH],\n",
        "                               train_y_cat, dev_cat_y,\n",
        "                               batch_size=500, epochs=10)\n",
        "time_start = time() - time_start\n",
        "\n",
        "print(\"Took : \"+str(np.round(time_start, 2))+\" (s)\") "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 6s 371ms/step - loss: 0.6279 - accuracy: 0.7688 - val_loss: 0.5042 - val_accuracy: 0.9550\n",
            "\n",
            "Testing loss: 0.5041759014129639, acc: 0.9549999833106995\n",
            "\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 5s 363ms/step - loss: 0.4603 - accuracy: 0.9487 - val_loss: 0.3747 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.37473323941230774, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 5s 367ms/step - loss: 0.3551 - accuracy: 0.9443 - val_loss: 0.2939 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.2939288020133972, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 5s 362ms/step - loss: 0.2844 - accuracy: 0.9501 - val_loss: 0.2487 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.24870163202285767, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 5s 359ms/step - loss: 0.2532 - accuracy: 0.9478 - val_loss: 0.2243 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.22430956363677979, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 5s 362ms/step - loss: 0.2326 - accuracy: 0.9484 - val_loss: 0.2094 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.20937475562095642, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 5s 364ms/step - loss: 0.2131 - accuracy: 0.9511 - val_loss: 0.1992 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.19916605949401855, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 5s 365ms/step - loss: 0.2054 - accuracy: 0.9501 - val_loss: 0.1917 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.1916774958372116, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 5s 362ms/step - loss: 0.2005 - accuracy: 0.9481 - val_loss: 0.1860 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.18595744669437408, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 5s 363ms/step - loss: 0.1914 - accuracy: 0.9495 - val_loss: 0.1816 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.1815880835056305, acc: 0.9559999704360962\n",
            "\n",
            "Took : 54.73 (s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt-l_Q9Aj44w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45507edc-458a-49d6-c556-ad0e1ed9b78a"
      },
      "source": [
        "history.history.keys()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xsxi8A1j_Fq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d59db165-b1b0-4413-c136-7fac4790e105"
      },
      "source": [
        "model.evaluate(x_dev[:, :MAX_TEXT_LENGTH], dev_cat_y, batch_size=1000)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 226ms/step - loss: 0.1816 - accuracy: 0.9560\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1815880984067917, 0.9559999704360962]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1e78k51kZ1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e783ecf4-5d85-4401-dd9f-d71015605fec"
      },
      "source": [
        "y_pred = np.argmax(model.predict(x_dev[:, :MAX_TEXT_LENGTH]), axis=1)\n",
        "\n",
        "print(creport(np.argmax(dev_cat_y, axis=1), y_pred,target_names=['HS', 'NOT_HS'],digits=3))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          HS      0.000     0.000     0.000        44\n",
            "      NOT_HS      0.956     1.000     0.978       956\n",
            "\n",
            "    accuracy                          0.956      1000\n",
            "   macro avg      0.478     0.500     0.489      1000\n",
            "weighted avg      0.914     0.956     0.934      1000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq27ypCT6dgw",
        "outputId": "6662dd8a-fbd6-42fe-e47b-b1e0da4ce7b4"
      },
      "source": [
        "model.evaluate(x_test[:, :MAX_TEXT_LENGTH], test_cat_y, batch_size=1000)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 212ms/step - loss: 0.1948 - accuracy: 0.9495\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.19483204185962677, 0.9495000243186951]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKxgtpLZ6dot",
        "outputId": "a3ec310d-ce4b-4153-85c4-6f996e9c53b5"
      },
      "source": [
        "y_pred = np.argmax(model.predict(x_test[:, :MAX_TEXT_LENGTH]), axis=1)\r\n",
        "\r\n",
        "print(creport(np.argmax(test_cat_y, axis=1), y_pred,target_names=['HS', 'NOT_HS'],digits=3))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          HS      0.000     0.000     0.000       101\n",
            "      NOT_HS      0.950     1.000     0.974      1899\n",
            "\n",
            "    accuracy                          0.950      2000\n",
            "   macro avg      0.475     0.500     0.487      2000\n",
            "weighted avg      0.902     0.950     0.925      2000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po5GA7KMkGUM"
      },
      "source": [
        "n = np.argmin(history.history['val_loss'])\n",
        "\n",
        "print(\"Optimal epoch : {}\".format(n))\n",
        "print(\"Accuracy on train : {} %\".format(np.round(history.history['accuracy'][n]*100, 2)))\n",
        "print(\"Accuracy on val : {} %\".format(np.round(history.history['val_accuracy'][n]*100, 2)))\n",
        "print(\"Loss on train : {}\".format(np.round(history.history['loss'][n]*100, 2)))\n",
        "print(\"Loss on Val : {}\".format(np.round(history.history['val_loss'][n]*100, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlVrg_zJkkLx"
      },
      "source": [
        "plt.figure(\"Loss Plot\", figsize=(12, 6))\n",
        "plt.plot(range(1, len(history.history['loss'])+1), history.history['loss'], label=\"train loss\")\n",
        "plt.plot(range(1, len(history.history['val_loss'])+1), history.history['val_loss'], label=\"val loss\")\n",
        "plt.plot(n+1,history.history[\"val_loss\"][n],\"r*\", label=\"Lowest loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.ylabel(\"loss (cross_entropy)\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZp0EW06kwhy"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='RNN_LSTM_model.png', show_shapes=False, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMeeL-Gqfy6p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}